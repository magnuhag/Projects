{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36e196b0",
   "metadata": {},
   "source": [
    "# Building a neural netowork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da397a61",
   "metadata": {},
   "source": [
    "In this notebook we will build a general purpose feed forward neural netowork. \n",
    "It is assumed that the reader is somewhat familiar with neural networks and is fluent in calculus. If not, a breif (and potentially un-enlightening) introduction to feed forward neural networks (ffnns) is presented in the following. The calculus I can't help you with. Suggested watching is 3Blue1Brown's video series (https://www.youtube.com/watch?v=aircAruvnKk&t=224s&ab_channel=3Blue1Brown) on neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfc666",
   "metadata": {},
   "source": [
    "Ffnns are baically a whole bunch of connected nodes (neurons) assembled in a layer configuration with an input layer, an arbitrary number of hidden layers, and an output layer. See illustration below:"
   ]
  },
  {
   "attachments": {
    "artificial-neural-network-architecture-ann-i-h-1-h-2-h-n-o-%281%29.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c2663683",
   "metadata": {},
   "source": [
    "![artificial-neural-network-architecture-ann-i-h-1-h-2-h-n-o-%281%29.png](attachment:artificial-neural-network-architecture-ann-i-h-1-h-2-h-n-o-%281%29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d9554",
   "metadata": {},
   "source": [
    "The networks work by feeding some data you want to model into the input layer, and through the magic of maths, a prediction is made in the output layer. I hope the introduction was enlightening. Now: onto the maths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9bb115",
   "metadata": {},
   "source": [
    "Each neuon in each (hidden) layer is connected to every neuron in the previous and consecutive layer. So if we input a value into Input 1, this value is sent to each neuron in the first hidden layer, and on the way undergoes a transformation. This is also the case for Input 2, and all inputs up to Input n. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4239ecf0",
   "metadata": {},
   "source": [
    "The connections between these neurons are called weights, and take numerical values. We will discuss these values in detail later. Each neuron also has an asscosiated bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45e169",
   "metadata": {},
   "source": [
    "Let's start to build a more mathematical formulation of this. We can begin my assesing what happens in neuron 1 in the first hidden layer when an array of data $\\mathbf{x}$ of length $n$ is fed into the input layer. Where $x_1$ enters Input 1 $x_2$ enters Input 2, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ebbb7",
   "metadata": {},
   "source": [
    "The values getting passed to neuron 1 is then $x_i$. But how much of $x_i$ \"enters\" neuron 1? For each value of $x_i$ we multiply by the weight asscosiated with the connection between the input layer and neuron 1. Mathematically, this is expressed as\n",
    "$$\n",
    "y_{in} = \\sum_{i=1}^n{w_ix_i}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c3032",
   "metadata": {},
   "source": [
    "When the data is recieved in the neuron, the bias of the neuron is added. \n",
    "$$\n",
    "y_{neuron} = \\sum_{i=1}^nw_ix_i +b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acc27d5",
   "metadata": {},
   "source": [
    "Now we take a small pause to talk about activation functions. These are functions that dictate how responsive the neuron is to the input. I'll get back to these functions later. At the moment the important thing to remember is that it is adventagous to limit or control how much the input affects the output of the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2c9555",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "With the activation function $f(z)$, the output (or activation) $a$ of the neuron becomes\n",
    "$$\n",
    "a = f\\biggr(\\sum_{i=1}^nw_ix_i +b\\biggr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620e293e",
   "metadata": {},
   "source": [
    "Now, we can generalize this for every neuron in the first hidden layer. Commence the index bonanza:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf2ccce",
   "metadata": {},
   "source": [
    "For neuron $i$ in layer $1$ (first hidden layer), the input is\n",
    "$$\n",
    "z_i^1=\\sum_{j = 1}^Mw_{ij}^1x_j+b_i^1\n",
    "$$\n",
    "where $M$ is the number of inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462b13d",
   "metadata": {},
   "source": [
    "The output of neuron $i$ then becomes\n",
    "$$\n",
    "a_i^1=f\\biggr(\\sum_{j = 1}^Mw_{ij}^1x_j+b_i^1\\biggr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ed6040",
   "metadata": {},
   "source": [
    "For the hidden layers $l=[2,3 , ..., L]$ - $L$ being the output layer - the output of of the $i$-th neuron becomes\n",
    "$$\n",
    "a_i^l=f\\biggr(\\sum_{j = 1}^{N_{l-1}} w_{ij}^l a_j^{l-1}+b_i^l\\biggr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c86f0cf",
   "metadata": {},
   "source": [
    "If we want we can have different activation functions for each layer, so that $a_i^l$ becomes\n",
    "$$\n",
    "a_i^l=f^l\\biggr(\\sum_{j = 1}^{N_{l-1}} w_{ij}^l a_j^{l-1}+b_i^l\\biggr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9299b1",
   "metadata": {},
   "source": [
    "Now this is all nice and well, but all these indices are honestly too much. Luckily, linear algebra comes to our aid (upper case variables being matrices, and lower case being vectors)\n",
    "$$\n",
    "a^1=f^1(X\\cdot W^1+\\mathbf{b}^1)\n",
    "\\tag{1}\n",
    "$$\n",
    "$$\n",
    "a^l = f^l(a^{l-1}\\cdot W^l+\\mathbf{b}^l)\n",
    "\\tag{2}\n",
    "$$\n",
    "where it is implied that the activation function is applied elementwise to $z$. This means that the activation function $f^l(z)$ is formally defined as\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{z}):\\mathbb{R}^n\\rightarrow \\mathbb{R}^n\n",
    "$$\n",
    "Feel free to call this forshadowing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb811a4",
   "metadata": {},
   "source": [
    "Now you might be thing \"Congratulations! You have managed to do absolutely nothing in terms of making a data model. And what actually are the weights and biases? WHY ARE WE DOING THIS?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af667b",
   "metadata": {},
   "source": [
    "This is a totally legitimate line of reasoning, and I'm going to pretty much ignore it. What I'll say is that our goal ultimately is to find weights and biases that will transform the input data into the desired data, i.e. make a prediction (that's actually correct) in the output layer. And don't worry fam, I'll get to that now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4a141",
   "metadata": {},
   "source": [
    "The output of the network is the activation of the neuron(s) in the last layer. We write this as\n",
    "$$\n",
    "\\tilde{y} = a^L\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81da9c7",
   "metadata": {},
   "source": [
    "Now, we might want to evaluate how the netowork performed. This is done with a cost function $C(W^L)$. The point of the cost function is to tell us how bad (or) good the netowork did. After feeding the data through the network once, you'd might excpect the prediction to be absolutely useless, and I think you should keep on to that intuition. But we can use this garbage prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db46ef0e",
   "metadata": {},
   "source": [
    "By finding the gradient of the cost function, we can update the weights and biases in such a way that the next prediction is better. Let's do some maths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3e8b65",
   "metadata": {},
   "source": [
    "We define a cost function $C(W^L)=C(f^L(a^{L-1}W^L+b^L))$. We also remind remember that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc263f8b",
   "metadata": {},
   "source": [
    "$$\n",
    "a^L=f^L(a^{L-1}W^L+b^L)=f^L(z^L)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a76249",
   "metadata": {},
   "source": [
    "We then find the derivative (assuming for the moment that all variables and \"functions\" are one-dimential, e.g. $C(W)=e^W,\\; \\; \\; W\\in\\mathbb{R}$) of the cost function\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial W^L}=\\frac{\\partial C}{\\partial a^L}\\frac{\\partial a^L}{\\partial z^L}\\frac{\\partial z^L}{\\partial W^L}\n",
    "=\\frac{\\partial C}{\\partial a^L}\\frac{\\partial a^L}{\\partial z^L}a^{L-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e0798",
   "metadata": {},
   "source": [
    "We write this (in matrix form) as\n",
    "$$\n",
    "\\nabla_{W^L}C= (a^{L-1})^T\\biggr( f'(z^L)\\odot\\nabla_{a^L}C \\biggr) =(a^{L-1})^T\\delta^L\n",
    "\\tag{3}\n",
    "$$\n",
    "where we define $\\delta^L$ as the error in the output layer. We're kinda doing matrix calculus here (not really, I'm just presenting you with a result), so it might be of value to note that we're getting some transposes and hadamard products all of a sudden, and the order of the products are mirrored. I'm not going into any detail about this, but it's easy (not really,  but it's not too hard to work through to show the same result) to see where these come from if you instead work with the explicit form\n",
    "$$\n",
    "C(W^L)=C\\biggr(f^L\\biggr(\\sum_{j = 1}^{N_{L-1}} W_{ij}^L a_j^{L-1}+b_i^L\\biggr)\\Biggr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed79081",
   "metadata": {},
   "source": [
    "You can also use this nifty website http://www.matrixcalculus.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b14687",
   "metadata": {},
   "source": [
    "We can find the error made by an arbitrary layer with (again assuming variables $\\in \\mathbb{R}$, not $\\mathbb{R}^{n\\times m}$)\n",
    "$$\n",
    "\\delta^l=\\frac{\\partial C}{\\partial z^l}=\\frac{\\partial C}{\\partial z^{l+1}}\\frac{\\partial z^{l+1}}{\\partial z^l}=\\delta^{l+1}\\frac{\\partial z^{l+1}}{\\partial z^l}\n",
    "$$\n",
    "Now we remember that\n",
    "$$\n",
    "z^{l+1}=a^lW^{l+1}+b^{l+1}\n",
    "$$\n",
    "thus\n",
    "$$\n",
    "\\frac{\\partial z^{l+1}}{\\partial z^l}=f'(z^l)W^{l+1}\n",
    "$$\n",
    "Yielding\n",
    "$$\n",
    "\\delta^l=\\delta^{l+1}f'(z^l)W^{l+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f031d45f",
   "metadata": {},
   "source": [
    "Or in matrix form\n",
    "$$\n",
    "\\delta^l=(\\delta^{l+1}(W^{l+1})^T)\\odot f'(z^l)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7318070b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\rightarrow \\nabla_{W^l}C=a^{l-1}\\delta^l\n",
    "\\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2687dc",
   "metadata": {},
   "source": [
    "By using gradient descent we can find the optimal weights for our network\n",
    "$$\n",
    "W^l\\leftarrow W^l-\\eta\\nabla_{W^l}C\n",
    "$$\n",
    "Where $\\eta$ is some tunable parameter we use to scale the gradient if it is too big, so that we don't overshoot the minumum of the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf85879",
   "metadata": {},
   "source": [
    "We can do the same thing with the bias bee seeing that\n",
    "$$\n",
    "\\delta^L=\\frac{\\partial C}{\\partial b^L}\\frac{\\partial b^L}{\\partial z^L}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc392a1",
   "metadata": {},
   "source": [
    "So now we can also update the biases\n",
    "$$\n",
    "b^l\\leftarrow b^l-\\eta\\delta^l\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b699458f",
   "metadata": {},
   "source": [
    "In summation, we have four equations we use to find the error of each layer, and update the weights and biases\n",
    "$$\n",
    "\\delta^L= f'(z^L)\\odot\\nabla_{a^L}C\n",
    "$$\n",
    "$$\n",
    "\\delta^l = (\\delta^{l+1}(W^{l+1})^T)\\odot f'(z^l)\n",
    "$$\n",
    "$$\n",
    "W^l\\leftarrow W^l-\\eta(a^{l-1})^T\\delta^l\n",
    "$$\n",
    "$$\n",
    "b^l\\leftarrow b^l-\\eta\\delta^l\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dddbca6",
   "metadata": {},
   "source": [
    "This is all well and nice, but we have run into a bit of potentially problematic notation here. That is, we (and pretty much every source I've found on the subject of backpropagation) write $f'(z^L)$, which is ambiguous at best. You might remember me mentioning earlier that we apply the activation function elementwise to the $z^l$ vectors. This means that we actually have\n",
    "$$\n",
    "f(z^L)=\\sum_{i}f(z_i^L)\\hat{e}_i=\\sum_{i}f_i\\hat{e}_i\n",
    "$$\n",
    "$\\hat{e}_i$ being a unit vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27271f1",
   "metadata": {},
   "source": [
    "Let's take the example where $f(x)=e^{x}$. We get\n",
    "$$\n",
    "f(\\vec{x})=e^{x_1}\\hat{e}_1+e^{x_2}\\hat{e}_2+\\cdots+e^{x_n}\\hat{e}_n\n",
    "$$\n",
    "and we have to apply the Jacobian matrix to evaluate any form of derivative of this function.\n",
    "$$\n",
    "\\mathcal{J}(f(z^L)=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial}{\\partial x_1}e^{x_1} & \\frac{\\partial}{\\partial x_2}e^{x_1} & \\cdots & \\frac{\\partial}{\\partial x_n}e^{x_1}\\\\\n",
    "\\frac{\\partial}{\\partial x_1}e^{x_2} & \\frac{\\partial}{\\partial x_2}e^{x_2} & \\cdots & \\frac{\\partial}{\\partial x_n}e^{x_2}\\\\\n",
    "\\vdots &  &\\ddots & \\vdots\\\\\n",
    "\\frac{\\partial}{\\partial x_1}e^{x_m} & \\frac{\\partial}{\\partial x_2}e^{x_m} & \\cdots & \\frac{\\partial}{\\partial x_n}e^{x_m}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Which obviously reduces to\n",
    "$$\n",
    "\\mathcal{J}(f(z^L))=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial }{\\partial x_1}e^{x_1} & 0 &\\cdots & 0 \\\\\n",
    "0 & \\frac{\\partial }{\\partial x_2}e^{x_2} & \\cdots & 0\\\\\n",
    "\\vdots & & \\ddots\\\\\n",
    "0 & 0 & \\cdots & \\frac{\\partial }{\\partial x_n}e^{x_n} \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325a4a4",
   "metadata": {},
   "source": [
    "We can see that for these kinds of functions, the derivative must be evaluated as the following Jacobian matrix\n",
    "$$\n",
    "\\mathcal{J}(f(z^L))=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f_1}{\\partial z_1^L} & 0 & \\cdots & 0 \\\\\n",
    "0 & \\frac{\\partial f_2}{\\partial z_2^L}  & \\cdots &0 \\\\\n",
    "\\vdots & & \\ddots \\\\\n",
    "0 & 0 & \\cdots & \\frac{\\partial f_n}{\\partial z_n^L} \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0548ae0",
   "metadata": {},
   "source": [
    "Or we can write \n",
    "$$\n",
    "\\mathcal{J}(f(z^L))_{ij}=\\delta_{ij}\\frac{\\partial f_i}{\\partial z_j^L}\n",
    "$$\n",
    "$\\delta_{ij}$ being the Kronecker delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ddf745",
   "metadata": {},
   "source": [
    "But there are other types of functions we will want to use, where this very nice behaviour does not emerge. Let's take a look at the softmax function $\\sigma(\\mathbf{z})_i$:\n",
    "$$\n",
    "\\sigma(\\mathbf{z})_i=\\frac{e^{\\mathbf{z}_i}}{\\sum_{k=1}^Me^{\\mathbf{z}_k}},\\;\\;\\; \\forall i =1, \\cdots, M\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33f98c6",
   "metadata": {},
   "source": [
    "We start by differentiating the i-th component w.r.t. $z_j$ (remembering the quotient rule):\n",
    "$$\n",
    "\\frac{\\partial \\sigma_i}{\\partial z_j}=\\frac{\\frac{\\partial}{\\partial z_j}e^{z_i}\\sum_ke^{z_k}-e^{z_i}\\frac{\\partial}{\\partial z_j}\\Biggr(\\sum_ke^{z_k}\\Biggr)}{\\Biggr(\\sum_ke^{z_k}\\Biggr)^2}\n",
    "$$\n",
    "We evaluate the partial derivatives in the fraction one by each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f0ccec",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial}{\\partial z_j}e^{z_i}=\\delta_{ij}e^{z_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0476280",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial}{\\partial z_j}\\Biggr(\\sum_ke^{z_k}\\Biggr)=e^{z_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7581faed",
   "metadata": {},
   "source": [
    "Which gives us\n",
    "$$\n",
    "\\frac{\\partial \\sigma_i}{\\partial z_j}=\\frac{\\delta_{ij}e^{z_i}\\sum_ke^{z_k}-e^{z_i}e^{z_j}}{\\Biggr(\\sum_ke^{z_k}\\Biggr)^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736950e9",
   "metadata": {},
   "source": [
    "$$\n",
    "=\\frac{e^{z_i}}{\\sum_ke^{e_k}}\\Biggr(\\frac{\\delta_{ij}\\sum_ke^{z_k}-e^{z_j}}{\\sum_k{e^{z_k}}}\\Biggr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c48b97",
   "metadata": {},
   "source": [
    "$$\n",
    "=\\sigma_i(\\delta_{ij}-\\sigma_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e32aa12",
   "metadata": {},
   "source": [
    "So we see that we in fact have to evaluate every element of the jacobian, since the off-diagonal elements are non-zero. But why make a big deal out of this? Because if the derivative of the activation function always were the diagonal Jacobian matrix, there would be a pretty obvious link between the expressions\n",
    "$$\n",
    "f'(z^L)\\odot\\nabla_{a^L}C\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\mathcal{J}(f(z^L))\\cdot \\nabla_{a^L}C\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50289fe",
   "metadata": {},
   "source": [
    "because\n",
    "$$\n",
    "\\mathcal{J}(f(x))\\cdot \\nabla _{a^L}C=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f_1}{\\partial z_1^L} & 0 & \\cdots & 0 \\\\\n",
    "0 & \\frac{\\partial f_2}{\\partial z_2^L}  & \\cdots &0 \\\\\n",
    "\\vdots & & \\ddots \\\\\n",
    "0 & 0 & \\cdots & \\frac{\\partial f_n}{\\partial z_n^L} \\\\\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\Biggr[\\frac{\\partial C}{\\partial a_1^L}, \\frac{\\partial C}{\\partial a_2^L},\\cdots, \\frac{\\partial C}{\\partial a_n^L}\\Biggr]^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a75542",
   "metadata": {},
   "source": [
    "$$\n",
    "=\\Biggr[\\frac{\\partial f_1}{\\partial x_1}\\frac{\\partial C}{\\partial a_1^L},\\cdots,\\frac{\\partial f_n}{\\partial x_n} \\frac{\\partial C}{\\partial a_n^L}\\Biggr]^T\\simeq f'(z^L)\\odot\\nabla_{a^L}C\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a2912a",
   "metadata": {},
   "source": [
    "To show that $f'(z^L)\\odot\\nabla_{a^L}C$ and $\\mathcal{J}(f(z^L))\\cdot \\nabla_{a^L}C$ are analogous expressions becomes much harder when the derivative of the activation function is\n",
    "$$\n",
    "\\mathcal{J}(f(z^L))=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "This is because the product $\\mathcal{J}(f(z^L))\\cdot \\nabla_{a^L}C$ becomes\n",
    "$$\n",
    "\\delta^L=\\biggr(\\sum_i^n\\frac{\\partial f_1}{\\partial z_i^L}\\frac{\\partial C}{\\partial a_i^L},\n",
    "\\sum_i^n\\frac{\\partial f_2}{\\partial z_i^L}\\frac{\\partial C}{\\partial a_i^L},\\cdots,\n",
    "\\sum_i^n\\frac{\\partial f_m}{\\partial z_i^L}\\frac{\\partial C}{\\partial a_i^L}\\biggr)\n",
    "$$\n",
    "and one can easily lose intuition for what it is that's going on. And to be honest, I'm not going to do much more to help you along the way. We must move on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5532794d",
   "metadata": {},
   "source": [
    "For computing these Jacobians and gradients ($\\mathcal{J}(f(z^L))$ and $\\nabla_{a^L}C$), we will use autograd. Autograd can calculate derivatives, gradients, and jacobians with phenomenal precision. A demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad0c2fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autograd in /Users/magnushagen/opt/anaconda3/lib/python3.7/site-packages (1.3)\n",
      "Requirement already satisfied: future>=0.15.2 in /Users/magnushagen/opt/anaconda3/lib/python3.7/site-packages (from autograd) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.12 in /Users/magnushagen/opt/anaconda3/lib/python3.7/site-packages (from autograd) (1.21.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2638ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1966881478808755e-18"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import jacobian \n",
    "from autograd import elementwise_grad as egrad\n",
    "\n",
    "#Defining the softmax\n",
    "def softmax(z):\n",
    "    return np.exp(z)/np.sum(np.exp(z))\n",
    "\n",
    "#Analytically found Jacobian of softmax\n",
    "def dsdz(z):\n",
    "    n = len(z)\n",
    "    arr = np.zeros((n, n))\n",
    "    s = softmax(z)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                delta_ij = 1\n",
    "            else:\n",
    "                delta_ij = 0\n",
    "            arr[i, j] = s[i]*(delta_ij-s[j])\n",
    "            \n",
    "    return arr\n",
    "#Random input values (z^L)\n",
    "z = np.random.randn(10)\n",
    "#Analytical solution\n",
    "dsdz_an = dsdz(z)\n",
    "#Numerical solution\n",
    "dsdz_auto = jacobian(softmax)(z)\n",
    "\n",
    "#average absolute difference \n",
    "np.mean(np.abs(dsdz_auto-dsdz_an))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76faf3a9",
   "metadata": {},
   "source": [
    "That should be pretty much everything I wanted to cover. What remains is writing the code for the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b776a",
   "metadata": {},
   "source": [
    "Now, I have cheated a bit, and I wrote the code for the neural network earlier. Here you go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff9bb4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        #Lists for holding the weight, bias, etc, matrices/ vectors.\n",
    "        #Call them (for the time being) \"empty\" tensors, if you're so inclined\n",
    "        self.layers = []\n",
    "        self.act_funcs = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.Z = []\n",
    "        self.A = []\n",
    "        self.delta = []\n",
    "\n",
    "    def add(self, n_neurons, act_func, input_size = None):\n",
    "\n",
    "        \"\"\"\n",
    "        Sequantially adds layer to network in the order (in, hidden_1, ..., hidden_n, out). When adding input layer,\n",
    "        input size must be specified.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(n_neurons, int) and n_neurons >= 1:\n",
    "            self.layers.append(n_neurons)\n",
    "\n",
    "        else:\n",
    "            #Should be obvious to anyone attempting to use this class. Still: might catch a typo\n",
    "            raise TypeError(\"n_neurons must be of type int and greater than or equal to 1\")\n",
    "\n",
    "        if isinstance(input_size, int):\n",
    "            #I haven't really discussed initialization of weights and biases. Upsies\n",
    "            self.weights.append(np.random.randn(input_size, n_neurons)*0.01)\n",
    "\n",
    "        elif isinstance(input_size, type(None)):\n",
    "            self.weights.append(np.random.randn(self.layers[-2], n_neurons)*0.01)\n",
    "        #Errrrr, I'll get back to this\n",
    "        else:\n",
    "            raise TypeError(\"Errr\")\n",
    "\n",
    "        if isinstance(act_func, str):\n",
    "            function = self.activation_function(act_func)\n",
    "            self.act_funcs.append(function)\n",
    "        else:\n",
    "            raise TypeError(\"act_func argument must be of type str\")\n",
    "\n",
    "        self.biases.append(np.random.randn(n_neurons)*0.01)\n",
    "\n",
    "        #For each added layer we append 0 to the lists\n",
    "        #so that they get the appropriate length\n",
    "        self.A.append(0)\n",
    "        self.Z.append(0)\n",
    "        self.delta.append(0)\n",
    "\n",
    "    def activation_function(self, act):\n",
    "        \"\"\"\n",
    "        Currently available activation functions:\n",
    "        \"simgoid\", \"RELU\", \"leaky_REALU\", \"softmax\", and \"linear\"\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if act == \"sigmoid\":\n",
    "            activ = lambda x: 1/(1+np.exp(-x))\n",
    "\n",
    "        elif act == \"RELU\":\n",
    "            activ = lambda x: np.maximum(x, 0)\n",
    "\n",
    "        elif act == \"leaky_RELU\":\n",
    "            activ = lambda x: np.maximum(x, 0.01 * x)\n",
    "\n",
    "        elif act == \"softmax\":\n",
    "            activ = lambda x: np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "        elif act == \"linear\":\n",
    "            activ = lambda x: x\n",
    "\n",
    "        #Yes, formatting\n",
    "        else:\n",
    "            print(\"-----------------------------------\")\n",
    "            print(\" \")\n",
    "            print(str(act) + \" is an invalid activation function name\")\n",
    "            print(\" \")\n",
    "            print(\"-----------------------------------\")\n",
    "\n",
    "            return\n",
    "\n",
    "        return activ\n",
    "\n",
    "    def loss_function(self, loss):\n",
    "        \"\"\"Currently available loss functions:\n",
    "        \"MSE\", and \"categorical_cross\"\"\"\n",
    "\n",
    "        if isinstance(loss, str):\n",
    "            if loss == \"MSE\":\n",
    "                func = lambda x, y: np.mean((x - y)**2, axis = 0, keepdims = True)\n",
    "            elif loss == \"categorical_cross\":\n",
    "                func = lambda x,y: -np.sum(y*np.log(x), axis = 0)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid loss function name\")\n",
    "        else:\n",
    "            raise TypeError(\"Loss function argument must be of type str\")\n",
    "\n",
    "        return func\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "\n",
    "        #Feeding in feature matrix\n",
    "        self.Z[0] = X @ self.weights[0] + self.biases[0].T\n",
    "        #Activation in first hidden layer\n",
    "        self.A[0] = self.act_funcs[0](self.Z[0])\n",
    "\n",
    "        for i in range(1, len(self.weights)):\n",
    "            #Feeding forward\n",
    "            self.Z[i] = self.A[i-1] @ self.weights[i] + self.biases[i].T\n",
    "            self.A[i] = self.act_funcs[i](self.Z[i])\n",
    "\n",
    "    def diff(self, C, A):\n",
    "\n",
    "        dCda = egrad(C)\n",
    "        dAdz = jacobian(A)\n",
    "\n",
    "        return dCda, dAdz\n",
    "\n",
    "    def back_prop(self, y, diff):\n",
    "\n",
    "            #Assigning Jacobian and gradient functions as variables\n",
    "            dC, da = diff\n",
    "            #\"Empty\" (Zeros) array to hold Jacobian\n",
    "            d_act = np.zeros(len(self.Z[-1]))\n",
    "            #Empty array to hold derivative of cost function\n",
    "            dcda = d_act.copy()\n",
    "            #Empty array to hold delta^L\n",
    "            self.delta[-1] = np.zeros((len(self.Z[-1]), self.layers[-1]))\n",
    "            #Calculate Jacobian and gradient for each training example in batch\n",
    "            for i in range(len(self.Z[-1])):\n",
    "                d_act = da(self.Z[-1][i])\n",
    "                dcda = dC(self.A[-1][i], y[i])\n",
    "                #Jacobian of activation times derivative of cost function\n",
    "                self.delta[-1][i] = d_act @ dcda\n",
    "                \n",
    "            #Backprop\n",
    "            for i in range(len(self.weights)-2, -1, -1):\n",
    "                #Gradient of activation function of hidden layer i. No need for Jacobian here\n",
    "                dfdz = egrad(self.act_funcs[i])\n",
    "                #Equation 2 is calculated in 2 parts. Just for ease of reading\n",
    "                t1 =  self.delta[i+1] @ self.weights[i+1].T\n",
    "                self.delta[i] = np.multiply(t1, dfdz(self.Z[i]))\n",
    "\n",
    "    def optimizer(self, X, eta):\n",
    "        \"\"\"\n",
    "        For the moment only supports mini-batch SGD. More will come (maybe)\n",
    "        \"\"\"\n",
    "\n",
    "        self.weights[0] -= eta * (X.T @ self.delta[0])\n",
    "        self.biases[0] -= eta * np.sum(self.delta[0], axis = 0)\n",
    "\n",
    "        for i in range(1, len(self.weights)):\n",
    "            self.weights[i] -= eta * (self.A[i-1].T @ self.delta[i])\n",
    "            self.biases[i] -= eta * np.sum(self.delta[i], axis = 0)\n",
    "\n",
    "    def train(self, X, y, epochs, loss, metric, batch_size = 10, num_iters = 100, eta_init = 10**(-4), decay = 0.1):\n",
    "\n",
    "        \"\"\"\n",
    "        args: X (feature matrix), y (targets), and epochs (type int).\n",
    "        kwargs: batch_size, num_iters, eta_init, decay. The \"standard\" values provided by the method\n",
    "        has been found by testing on one dataset. You should probably not use the values Ive found.\n",
    "        \"\"\"\n",
    "\n",
    "        diff = self.diff(self.loss_function(loss), self.act_funcs[-1])\n",
    "\n",
    "        data_indices = len(X)\n",
    "        #eta function (not the Dirichlet one): for decreasing learning rate as training progresses\n",
    "        eta = lambda eta_init, iteration, decay: eta_init/(1+decay*iteration)\n",
    "\n",
    "        for i in range(1, epochs+1):\n",
    "            for j in range(num_iters):\n",
    "                eta1 = eta(eta_init, j, decay)\n",
    "                #Randomly choose datapoints to use as mini-batches\n",
    "                chosen_datapoints = np.random.choice(data_indices, size = batch_size, replace = False)\n",
    "                #Making mini-batches\n",
    "                X_mini = X[chosen_datapoints]\n",
    "                y_mini = y[chosen_datapoints]\n",
    "                #Feed forward\n",
    "                self.feed_forward(X_mini)\n",
    "                #Backprop\n",
    "                self.back_prop(y_mini, diff)\n",
    "                #Update weights and biases\n",
    "                self.optimizer(X_mini, eta(eta_init, j, decay))\n",
    "\n",
    "            #Make a prediction and print mean of performance and loss of mini-batch\n",
    "            predicted = self.predict(X_mini)\n",
    "            metric_val = np.mean(self.metrics(predicted, y_mini, metric))\n",
    "            loss_val = np.mean(self.loss_function(loss)(predicted, y_mini))\n",
    "            #Yes, formatting\n",
    "            print(\"mean loss = \" + str(loss_val) +\" -------------- \" + metric + \" = \" + str(metric_val) + \" at epoch \" +str(i))\n",
    "\n",
    "    def metrics(self, y_hat, y, a):\n",
    "        \"\"\"\n",
    "        Takes args: y_hat, y, a (prediction, targets, activation in layer L)\n",
    "        Currently available metrics:\n",
    "        \"accuracy\", and \"MSE\"\n",
    "        \"\"\"\n",
    "        if a == \"accuracy\":\n",
    "            s = 0\n",
    "            for i in range(len(y)):\n",
    "                true = np.argmax(y[i])\n",
    "                pred = np.argmax(y_hat[i])\n",
    "                if true == pred:\n",
    "                    s += 1\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            return s/len(y_hat)\n",
    "\n",
    "        elif a == \"MSE\":\n",
    "            return np.mean((y-y_hat)**2, axis = 0)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Takes arg: X\n",
    "        Does one feed forward pass and returns the output of last layer\n",
    "        \"\"\"\n",
    "        self.feed_forward(X)\n",
    "        return self.A[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c973b-be2c-4870-8617-147dd6519f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "474237e7",
   "metadata": {},
   "source": [
    "Now, let's prepare the data and train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ea4e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.00862078 -0.         -0.         13.50389001  4.66485356  4.60058986\n",
      "  9.04237517 -0.         -0.          4.62440488]\n",
      "mean loss = 4.544473426332749 -------------- accuracy = 0.5 at epoch 1\n",
      "[ 4.46981398 -0.         -0.         -0.         -0.          4.60194031\n",
      "  9.05764845 13.96947615 -0.         13.89859567]\n",
      "mean loss = 4.59974745618497 -------------- accuracy = 0.1 at epoch 2\n",
      "[ 4.50257434 13.58171781 -0.         -0.          9.06930215  4.60179306\n",
      " -0.          9.21717725  4.61607527 -0.        ]\n",
      "mean loss = 4.558863988035406 -------------- accuracy = 0.1 at epoch 3\n",
      "[13.5383415   4.52847646 -0.         -0.          9.07498237  4.63092185\n",
      " -0.         -0.          4.66174078  9.13381297]\n",
      "mean loss = 4.556827593780314 -------------- accuracy = 0.5 at epoch 4\n",
      "[-0.          4.55883777  9.23489764  4.42957873  9.00513346  4.63685657\n",
      " -0.          4.61492644 -0.          9.04093712]\n",
      "mean loss = 4.552116772301351 -------------- accuracy = 0.2 at epoch 5\n",
      "[-0.          9.14107077 -0.         -0.         -0.         -0.\n",
      "  8.91435265 -0.         13.57214106 13.29898367]\n",
      "mean loss = 4.492654815259311 -------------- accuracy = 0.6 at epoch 6\n",
      "[ 4.38727908 -0.          4.52457866 -0.          8.82427297  4.68276568\n",
      "  8.828955    4.62786894  9.03579096 -0.        ]\n",
      "mean loss = 4.491151127873048 -------------- accuracy = 0.7 at epoch 7\n",
      "[-0.         -0.          4.54016022  8.73596232  8.76171616  4.59763245\n",
      " -0.          9.07359488  4.56527973  4.31787291]\n",
      "mean loss = 4.459221866773666 -------------- accuracy = 0.6 at epoch 8\n",
      "[ 4.07816559  4.48034737 -0.          4.31903633  8.52258401  9.14732162\n",
      " -0.          9.02471775  4.52971689 -0.        ]\n",
      "mean loss = 4.410188955505822 -------------- accuracy = 0.5 at epoch 9\n",
      "[ 8.00123555 -0.         -0.          8.37774881 -0.          9.14039721\n",
      "  4.20256547 -0.          4.57703682  9.21361305]\n",
      "mean loss = 4.3512596917096005 -------------- accuracy = 0.6 at epoch 10\n",
      "[-0.         -0.          8.75156205  4.05644291  4.17829447  4.57699126\n",
      "  4.07889698  4.25802536  9.29886332  4.15724128]\n",
      "mean loss = 4.335631763383561 -------------- accuracy = 0.4 at epoch 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fdd602a3f2a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#We're leaving batch size at a modest 10 as to not having to spend all day training the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"categorical_cross\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mt_train_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-17e58a0859b5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, epochs, loss, metric, batch_size, num_iters, eta_init, decay)\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m#Backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0;31m#Update weights and biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-17e58a0859b5>\u001b[0m in \u001b[0;36mback_prop\u001b[0;34m(self, y, diff)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m#Calculate Jacobian and gradient for each training example in batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0md_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0mdcda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m#Jacobian of activation times derivative of cost function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/autograd/wrap_util.py\u001b[0m in \u001b[0;36mnary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0munary_operator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munary_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnary_op_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnary_op_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnary_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnary_operator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/autograd/differential_operators.py\u001b[0m in \u001b[0;36mjacobian\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mjacobian_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mans_vspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans_vspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard_basis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjacobian_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0munary_to_nary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/autograd/numpy/numpy_wrapper.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# primitives defined in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'need at least one array to stack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/autograd/numpy/numpy_wrapper.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# primitives defined in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'need at least one array to stack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/autograd/core.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/autograd/core.py\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(g, end_node)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoposort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mingrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moutgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_outgrads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/autograd/core.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mvjp_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_0_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mvjp_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_1_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvjp_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvjp_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mvjps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvjps_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0margnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/autograd/numpy/numpy_vjps.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munbroadcast_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0mtarget_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0munbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munbroadcast_einsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/autograd/numpy/numpy_vjps.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     50\u001b[0m defvjp(anp.logaddexp2,  lambda ans, x, y : unbroadcast_f(x, lambda g: g * 2**(x-ans)),\n\u001b[1;32m     51\u001b[0m                         lambda ans, x, y : unbroadcast_f(y, lambda g: g * 2**(y-ans)))\n\u001b[0;32m---> 52\u001b[0;31m defvjp(anp.true_divide, lambda ans, x, y : unbroadcast_f(x, lambda g: g / y),\n\u001b[0m\u001b[1;32m     53\u001b[0m                         lambda ans, x, y : unbroadcast_f(y, lambda g: - g * x / y**2))\n\u001b[1;32m     54\u001b[0m defvjp(anp.mod,         lambda ans, x, y : unbroadcast_f(x, lambda g: g),\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import perf_counter\n",
    "\n",
    "def fix_data():\n",
    "    # download MNIST dataset\n",
    "    digits = datasets.load_digits()\n",
    "\n",
    "    # define inputs and labels\n",
    "    inputs = digits.images\n",
    "    labels = digits.target\n",
    "\n",
    "    # one-hot encoding the targest\n",
    "    def to_categorical_numpy(integer_vector):\n",
    "        n_inputs = len(integer_vector)\n",
    "        n_categories = np.max(integer_vector) + 1\n",
    "        onehot_vector = np.zeros((n_inputs, n_categories))\n",
    "        onehot_vector[range(n_inputs), integer_vector] = 1\n",
    "        \n",
    "        return onehot_vector\n",
    "\n",
    "\n",
    "    n_inputs = len(inputs)\n",
    "    inputs = inputs.reshape(n_inputs, -1)\n",
    "    X = inputs\n",
    "    Y = to_categorical_numpy(labels)\n",
    "    return X, Y\n",
    "\n",
    "X, Y = fix_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2 )\n",
    "\n",
    "in_size = len(X[0])\n",
    "out_size = len(y_test[0])\n",
    "\n",
    "Net = NeuralNet()\n",
    "#We don't need anything fancy for this demonstration. 1 hidden layer is enough\n",
    "Net.add(in_size, \"sigmoid\", input_size = in_size)\n",
    "Net.add(10, \"softmax\")\n",
    "\n",
    "t_train_start = perf_counter()\n",
    "\n",
    "#We're leaving batch size at a modest 10 as to not having to spend all day training the network \n",
    "Net.train(X_train, y_train, 100, \"categorical_cross\", \"accuracy\", batch_size = 10, num_iters = 100)\n",
    "\n",
    "t_train_stop = perf_counter()\n",
    "\n",
    "print(t_train_stop-t_train_start)\n",
    "\n",
    "#Reminder of args and kwargs\n",
    "#train(self, X, y, epochs, loss, metric, batch_size = 10, num_iters = 50, eta_init = 10**(-4), decay = 0.1)\n",
    "\n",
    "\n",
    "pred = Net.predict(X_test)\n",
    "s = 0\n",
    "for i in range(len(X_test)):\n",
    "    true = np.argmax(y_test[i])\n",
    "    guess = np.argmax(pred[i])\n",
    "    if true == guess:\n",
    "        s += 1\n",
    "print(\"test accuracy is \" + str(s/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f087625",
   "metadata": {},
   "source": [
    "We get a good test accuracy (\\~98%), but this takes forever (\\~188s). There are of course stuff that could be done to improve speed of execution. Now lets do the same with Tensorflow (architecture is a bit different, but whatevs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cde7811e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1437/1437 [==============================] - 0s 59us/sample - loss: 2.1619\n",
      "Epoch 2/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 1.8226\n",
      "Epoch 3/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 1.6169\n",
      "Epoch 4/400\n",
      "1437/1437 [==============================] - 0s 24us/sample - loss: 1.4564\n",
      "Epoch 5/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 1.3196\n",
      "Epoch 6/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 1.2034\n",
      "Epoch 7/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 1.1034\n",
      "Epoch 8/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 1.0155\n",
      "Epoch 9/400\n",
      "1437/1437 [==============================] - 0s 23us/sample - loss: 0.9385\n",
      "Epoch 10/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.8697\n",
      "Epoch 11/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.8090\n",
      "Epoch 12/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.7556\n",
      "Epoch 13/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.7078\n",
      "Epoch 14/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.6647\n",
      "Epoch 15/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.6257\n",
      "Epoch 16/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.5918\n",
      "Epoch 17/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.5605\n",
      "Epoch 18/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.5323\n",
      "Epoch 19/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.5069\n",
      "Epoch 20/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.4834\n",
      "Epoch 21/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.4621\n",
      "Epoch 22/400\n",
      "1437/1437 [==============================] - 0s 22us/sample - loss: 0.4422\n",
      "Epoch 23/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.4243\n",
      "Epoch 24/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.4071\n",
      "Epoch 25/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.3918\n",
      "Epoch 26/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.3773\n",
      "Epoch 27/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.3642\n",
      "Epoch 28/400\n",
      "1437/1437 [==============================] - 0s 25us/sample - loss: 0.3514\n",
      "Epoch 29/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.3400\n",
      "Epoch 30/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.3289\n",
      "Epoch 31/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.3184\n",
      "Epoch 32/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.3088\n",
      "Epoch 33/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.2999\n",
      "Epoch 34/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.2917\n",
      "Epoch 35/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.2831\n",
      "Epoch 36/400\n",
      "1437/1437 [==============================] - 0s 22us/sample - loss: 0.2762\n",
      "Epoch 37/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.2690\n",
      "Epoch 38/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.2620\n",
      "Epoch 39/400\n",
      "1437/1437 [==============================] - 0s 25us/sample - loss: 0.2556\n",
      "Epoch 40/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.2494\n",
      "Epoch 41/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.2437\n",
      "Epoch 42/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.2383\n",
      "Epoch 43/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.2328\n",
      "Epoch 44/400\n",
      "1437/1437 [==============================] - 0s 23us/sample - loss: 0.2278\n",
      "Epoch 45/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.2231\n",
      "Epoch 46/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.2186\n",
      "Epoch 47/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.2141\n",
      "Epoch 48/400\n",
      "1437/1437 [==============================] - 0s 23us/sample - loss: 0.2101\n",
      "Epoch 49/400\n",
      "1437/1437 [==============================] - 0s 26us/sample - loss: 0.2059\n",
      "Epoch 50/400\n",
      "1437/1437 [==============================] - 0s 29us/sample - loss: 0.2022\n",
      "Epoch 51/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.1982\n",
      "Epoch 52/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.1948\n",
      "Epoch 53/400\n",
      "1437/1437 [==============================] - 0s 22us/sample - loss: 0.1912\n",
      "Epoch 54/400\n",
      "1437/1437 [==============================] - 0s 22us/sample - loss: 0.1878\n",
      "Epoch 55/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.1847\n",
      "Epoch 56/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.1816\n",
      "Epoch 57/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.1786\n",
      "Epoch 58/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.1758\n",
      "Epoch 59/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.1728\n",
      "Epoch 60/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.1701\n",
      "Epoch 61/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.1673\n",
      "Epoch 62/400\n",
      "1437/1437 [==============================] - 0s 22us/sample - loss: 0.1649\n",
      "Epoch 63/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.1625\n",
      "Epoch 64/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.1599\n",
      "Epoch 65/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.1576\n",
      "Epoch 66/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.1555\n",
      "Epoch 67/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.1531\n",
      "Epoch 68/400\n",
      "1437/1437 [==============================] - 0s 22us/sample - loss: 0.1510\n",
      "Epoch 69/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.1491\n",
      "Epoch 70/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.1469\n",
      "Epoch 71/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.1447\n",
      "Epoch 72/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.1432\n",
      "Epoch 73/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.1411\n",
      "Epoch 74/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.1393\n",
      "Epoch 75/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.1376\n",
      "Epoch 76/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.1358\n",
      "Epoch 77/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.1341\n",
      "Epoch 78/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.1324\n",
      "Epoch 79/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.1308\n",
      "Epoch 80/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.1292\n",
      "Epoch 81/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.1277\n",
      "Epoch 82/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.1263\n",
      "Epoch 83/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.1247\n",
      "Epoch 84/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.1233\n",
      "Epoch 85/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.1220\n",
      "Epoch 86/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.1204\n",
      "Epoch 87/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.1191\n",
      "Epoch 88/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.1178\n",
      "Epoch 89/400\n",
      "1437/1437 [==============================] - 0s 23us/sample - loss: 0.1167\n",
      "Epoch 90/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.1153\n",
      "Epoch 91/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.1141\n",
      "Epoch 92/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.1128\n",
      "Epoch 93/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.1116\n",
      "Epoch 94/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.1105\n",
      "Epoch 95/400\n",
      "1437/1437 [==============================] - 0s 24us/sample - loss: 0.1095\n",
      "Epoch 96/400\n",
      "1437/1437 [==============================] - 0s 24us/sample - loss: 0.1084\n",
      "Epoch 97/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.1071\n",
      "Epoch 98/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.1061\n",
      "Epoch 99/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.1052\n",
      "Epoch 100/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.1041\n",
      "Epoch 101/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.1031\n",
      "Epoch 102/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.1021\n",
      "Epoch 103/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.1010\n",
      "Epoch 104/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.1001\n",
      "Epoch 105/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0992\n",
      "Epoch 106/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0982\n",
      "Epoch 107/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0974\n",
      "Epoch 108/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0965\n",
      "Epoch 109/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0956\n",
      "Epoch 110/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0948\n",
      "Epoch 111/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0939\n",
      "Epoch 112/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0931\n",
      "Epoch 113/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0922\n",
      "Epoch 114/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0914\n",
      "Epoch 115/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.0907\n",
      "Epoch 116/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0898\n",
      "Epoch 117/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0891\n",
      "Epoch 118/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0884\n",
      "Epoch 119/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0877\n",
      "Epoch 120/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0870\n",
      "Epoch 121/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0861\n",
      "Epoch 122/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0855\n",
      "Epoch 123/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0848\n",
      "Epoch 124/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0842\n",
      "Epoch 125/400\n",
      "1437/1437 [==============================] - 0s 22us/sample - loss: 0.0834\n",
      "Epoch 126/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0827\n",
      "Epoch 127/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0821\n",
      "Epoch 128/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0814\n",
      "Epoch 129/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0808\n",
      "Epoch 130/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0802\n",
      "Epoch 131/400\n",
      "1437/1437 [==============================] - 0s 22us/sample - loss: 0.0795\n",
      "Epoch 132/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0789\n",
      "Epoch 133/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0784\n",
      "Epoch 134/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0778\n",
      "Epoch 135/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0771\n",
      "Epoch 136/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0765\n",
      "Epoch 137/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0760\n",
      "Epoch 138/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.0755\n",
      "Epoch 139/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0750\n",
      "Epoch 140/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0744\n",
      "Epoch 141/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.0737\n",
      "Epoch 142/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0734\n",
      "Epoch 143/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0728\n",
      "Epoch 144/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0723\n",
      "Epoch 145/400\n",
      "1437/1437 [==============================] - 0s 22us/sample - loss: 0.0718\n",
      "Epoch 146/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0713\n",
      "Epoch 147/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0707\n",
      "Epoch 148/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0703\n",
      "Epoch 149/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0698\n",
      "Epoch 150/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0692\n",
      "Epoch 151/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0688\n",
      "Epoch 152/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0684\n",
      "Epoch 153/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0679\n",
      "Epoch 154/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0674\n",
      "Epoch 155/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0670\n",
      "Epoch 156/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0666\n",
      "Epoch 157/400\n",
      "1437/1437 [==============================] - 0s 22us/sample - loss: 0.0661\n",
      "Epoch 158/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0657\n",
      "Epoch 159/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0653\n",
      "Epoch 160/400\n",
      "1437/1437 [==============================] - 0s 25us/sample - loss: 0.0648\n",
      "Epoch 161/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0644\n",
      "Epoch 162/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0639\n",
      "Epoch 163/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0636\n",
      "Epoch 164/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0631\n",
      "Epoch 165/400\n",
      "1437/1437 [==============================] - 0s 25us/sample - loss: 0.0628\n",
      "Epoch 166/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.0624\n",
      "Epoch 167/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0620\n",
      "Epoch 168/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0617\n",
      "Epoch 169/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0612\n",
      "Epoch 170/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0609\n",
      "Epoch 171/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.0605\n",
      "Epoch 172/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0601\n",
      "Epoch 173/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0598\n",
      "Epoch 174/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0594\n",
      "Epoch 175/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0590\n",
      "Epoch 176/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0587\n",
      "Epoch 177/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0584\n",
      "Epoch 178/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0580\n",
      "Epoch 179/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0577\n",
      "Epoch 180/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0574\n",
      "Epoch 181/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.0569\n",
      "Epoch 182/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0566\n",
      "Epoch 183/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0563\n",
      "Epoch 184/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0559\n",
      "Epoch 185/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.0557\n",
      "Epoch 186/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0553\n",
      "Epoch 187/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0550\n",
      "Epoch 188/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0547\n",
      "Epoch 189/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0544\n",
      "Epoch 190/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0541\n",
      "Epoch 191/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0538\n",
      "Epoch 192/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0535\n",
      "Epoch 193/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0532\n",
      "Epoch 194/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0530\n",
      "Epoch 195/400\n",
      "1437/1437 [==============================] - 0s 26us/sample - loss: 0.0526\n",
      "Epoch 196/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0523\n",
      "Epoch 197/400\n",
      "1437/1437 [==============================] - 0s 23us/sample - loss: 0.0521\n",
      "Epoch 198/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0518\n",
      "Epoch 199/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0515\n",
      "Epoch 200/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0512\n",
      "Epoch 201/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0510\n",
      "Epoch 202/400\n",
      "1437/1437 [==============================] - 0s 24us/sample - loss: 0.0507\n",
      "Epoch 203/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0504\n",
      "Epoch 204/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0502\n",
      "Epoch 205/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0499\n",
      "Epoch 206/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0497\n",
      "Epoch 207/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.0494\n",
      "Epoch 208/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0491\n",
      "Epoch 209/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0489\n",
      "Epoch 210/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0486\n",
      "Epoch 211/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0484\n",
      "Epoch 212/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0482\n",
      "Epoch 213/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0479\n",
      "Epoch 214/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0476\n",
      "Epoch 215/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0474\n",
      "Epoch 216/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0472\n",
      "Epoch 217/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0470\n",
      "Epoch 218/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0467\n",
      "Epoch 219/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0465\n",
      "Epoch 220/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0463\n",
      "Epoch 221/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0461\n",
      "Epoch 222/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0458\n",
      "Epoch 223/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0456\n",
      "Epoch 224/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0454\n",
      "Epoch 225/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0451\n",
      "Epoch 226/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0449\n",
      "Epoch 227/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0448\n",
      "Epoch 228/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0445\n",
      "Epoch 229/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0443\n",
      "Epoch 230/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0441\n",
      "Epoch 231/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0439\n",
      "Epoch 232/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0437\n",
      "Epoch 233/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0435\n",
      "Epoch 234/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0433\n",
      "Epoch 235/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0431\n",
      "Epoch 236/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0429\n",
      "Epoch 237/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.0427\n",
      "Epoch 238/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0425\n",
      "Epoch 239/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0423\n",
      "Epoch 240/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0421\n",
      "Epoch 241/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0419\n",
      "Epoch 242/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0417\n",
      "Epoch 243/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0416\n",
      "Epoch 244/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0414\n",
      "Epoch 245/400\n",
      "1437/1437 [==============================] - 0s 25us/sample - loss: 0.0412\n",
      "Epoch 246/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0410\n",
      "Epoch 247/400\n",
      "1437/1437 [==============================] - 0s 22us/sample - loss: 0.0407\n",
      "Epoch 248/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.0407\n",
      "Epoch 249/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0405\n",
      "Epoch 250/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0403\n",
      "Epoch 251/400\n",
      "1437/1437 [==============================] - 0s 23us/sample - loss: 0.0401\n",
      "Epoch 252/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0399\n",
      "Epoch 253/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0398\n",
      "Epoch 254/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0396\n",
      "Epoch 255/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0394\n",
      "Epoch 256/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0393\n",
      "Epoch 257/400\n",
      "1437/1437 [==============================] - 0s 23us/sample - loss: 0.0391\n",
      "Epoch 258/400\n",
      "1437/1437 [==============================] - 0s 22us/sample - loss: 0.0389\n",
      "Epoch 259/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0388\n",
      "Epoch 260/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0386\n",
      "Epoch 261/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0384\n",
      "Epoch 262/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0383\n",
      "Epoch 263/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0380\n",
      "Epoch 264/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0380\n",
      "Epoch 265/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0378\n",
      "Epoch 266/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0376\n",
      "Epoch 267/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0375\n",
      "Epoch 268/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0374\n",
      "Epoch 269/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.0372\n",
      "Epoch 270/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0370\n",
      "Epoch 271/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0369\n",
      "Epoch 272/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0368\n",
      "Epoch 273/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0366\n",
      "Epoch 274/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.0364\n",
      "Epoch 275/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0363\n",
      "Epoch 276/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0362\n",
      "Epoch 277/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0360\n",
      "Epoch 278/400\n",
      "1437/1437 [==============================] - 0s 21us/sample - loss: 0.0359\n",
      "Epoch 279/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0357\n",
      "Epoch 280/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0356\n",
      "Epoch 281/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0354\n",
      "Epoch 282/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0353\n",
      "Epoch 283/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0352\n",
      "Epoch 284/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0350\n",
      "Epoch 285/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0349\n",
      "Epoch 286/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0348\n",
      "Epoch 287/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0347\n",
      "Epoch 288/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0345\n",
      "Epoch 289/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0344\n",
      "Epoch 290/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0343\n",
      "Epoch 291/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0341\n",
      "Epoch 292/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0341\n",
      "Epoch 293/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0339\n",
      "Epoch 294/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0338\n",
      "Epoch 295/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0336\n",
      "Epoch 296/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0335\n",
      "Epoch 297/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0334\n",
      "Epoch 298/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0333\n",
      "Epoch 299/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0331\n",
      "Epoch 300/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0330\n",
      "Epoch 301/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0329\n",
      "Epoch 302/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0328\n",
      "Epoch 303/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0327\n",
      "Epoch 304/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0325\n",
      "Epoch 305/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0324\n",
      "Epoch 306/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0323\n",
      "Epoch 307/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0322\n",
      "Epoch 308/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0321\n",
      "Epoch 309/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0320\n",
      "Epoch 310/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0319\n",
      "Epoch 311/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0318\n",
      "Epoch 312/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0316\n",
      "Epoch 313/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0315\n",
      "Epoch 314/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0314\n",
      "Epoch 315/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0313\n",
      "Epoch 316/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0312\n",
      "Epoch 317/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0311\n",
      "Epoch 318/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0310\n",
      "Epoch 319/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0309\n",
      "Epoch 320/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0308\n",
      "Epoch 321/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0307\n",
      "Epoch 322/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0306\n",
      "Epoch 323/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0305\n",
      "Epoch 324/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0304\n",
      "Epoch 325/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0303\n",
      "Epoch 326/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0301\n",
      "Epoch 327/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0300\n",
      "Epoch 328/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0299\n",
      "Epoch 329/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0299\n",
      "Epoch 330/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0297\n",
      "Epoch 331/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0297\n",
      "Epoch 332/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0295\n",
      "Epoch 333/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0295\n",
      "Epoch 334/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0294\n",
      "Epoch 335/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0293\n",
      "Epoch 336/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0292\n",
      "Epoch 337/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0291\n",
      "Epoch 338/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0290\n",
      "Epoch 339/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0289\n",
      "Epoch 340/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0288\n",
      "Epoch 341/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0287\n",
      "Epoch 342/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0286\n",
      "Epoch 343/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0285\n",
      "Epoch 344/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0284\n",
      "Epoch 345/400\n",
      "1437/1437 [==============================] - 0s 15us/sample - loss: 0.0284\n",
      "Epoch 346/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0282\n",
      "Epoch 347/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0282\n",
      "Epoch 348/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0281\n",
      "Epoch 349/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0280\n",
      "Epoch 350/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0279\n",
      "Epoch 351/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0278\n",
      "Epoch 352/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0277\n",
      "Epoch 353/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0276\n",
      "Epoch 354/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0276\n",
      "Epoch 355/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0275\n",
      "Epoch 356/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0274\n",
      "Epoch 357/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0273\n",
      "Epoch 358/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0272\n",
      "Epoch 359/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0271\n",
      "Epoch 360/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0270\n",
      "Epoch 361/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0269\n",
      "Epoch 362/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0269\n",
      "Epoch 363/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0268\n",
      "Epoch 364/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0267\n",
      "Epoch 365/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0266\n",
      "Epoch 366/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0266\n",
      "Epoch 367/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0265\n",
      "Epoch 368/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0264\n",
      "Epoch 369/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0263\n",
      "Epoch 370/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0262\n",
      "Epoch 371/400\n",
      "1437/1437 [==============================] - 0s 18us/sample - loss: 0.0262\n",
      "Epoch 372/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0261\n",
      "Epoch 373/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0260\n",
      "Epoch 374/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0259\n",
      "Epoch 375/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0258\n",
      "Epoch 376/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0258\n",
      "Epoch 377/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0257\n",
      "Epoch 378/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0256\n",
      "Epoch 379/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0255\n",
      "Epoch 380/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0255\n",
      "Epoch 381/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0254\n",
      "Epoch 382/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0253\n",
      "Epoch 383/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0252\n",
      "Epoch 384/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0252\n",
      "Epoch 385/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0251\n",
      "Epoch 386/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0250\n",
      "Epoch 387/400\n",
      "1437/1437 [==============================] - 0s 17us/sample - loss: 0.0249\n",
      "Epoch 388/400\n",
      "1437/1437 [==============================] - 0s 20us/sample - loss: 0.0249\n",
      "Epoch 389/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0248\n",
      "Epoch 390/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0247\n",
      "Epoch 391/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0247\n",
      "Epoch 392/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0246\n",
      "Epoch 393/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0245\n",
      "Epoch 394/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0245\n",
      "Epoch 395/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0244\n",
      "Epoch 396/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0243\n",
      "Epoch 397/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0243\n",
      "Epoch 398/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0242\n",
      "Epoch 399/400\n",
      "1437/1437 [==============================] - 0s 19us/sample - loss: 0.0241\n",
      "Epoch 400/400\n",
      "1437/1437 [==============================] - 0s 16us/sample - loss: 0.0241\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense#, Flatten\n",
    "\n",
    "input_dim = len(X_train[0,:])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units = input_dim, activation = \"sigmoid\", input_dim = input_dim))\n",
    "\n",
    "model.add(Dense(units = 10, activation = \"softmax\"))\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"sgd\")\n",
    "t_train_start_tf = perf_counter()\n",
    "model.fit(X_train, y_train, epochs = 400, batch_size = 32)\n",
    "t_train_stop_tf = perf_counter()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acd01d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.005988005000063\n",
      "test accuracy is 0.9638888888888889\n"
     ]
    }
   ],
   "source": [
    "print(t_train_stop_tf-t_train_start_tf)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "s = 0\n",
    "for i in range(len(X_test)):\n",
    "    true = np.argmax(y_test[i])\n",
    "    guess = np.argmax(y_hat[i])\n",
    "    if true == guess:\n",
    "        s += 1\n",
    "print(\"test accuracy is \" + str(s/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b10c25",
   "metadata": {},
   "source": [
    "Wow! Better accuracy and speed. Not to mention it took me only a minute to cook up the Tensorflow code, whereas my ffnn took way longer.\n",
    "<br>\n",
    "At the end of the day, this may seem like an exercise of futility, but I believe its good to know how NNs work and how to code them (albeit primitively), before you start using Tensorflow or something similar.\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "Peace out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591dfa7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
